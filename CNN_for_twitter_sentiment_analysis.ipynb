{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPzUpNObtatKh7+Go6Xx6f2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ceydab/Python-Practice-Applications/blob/main/CNN_for_twitter_sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRS1kMNIsEXM",
        "outputId": "7e2dff19-b5af-411d-ae34-db74a003b82d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'tweeteval' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/cardiffnlp/tweeteval.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "import torch.utils.data as data"
      ],
      "metadata": {
        "id": "4u3XVspUt1yq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use the GPU\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
        "    print('Device name:', torch.cuda.get_device_name(0))\n",
        "\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "QFgaQ6Yb0JU1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5592ea57-7578-4b4f-f1cd-15bf2ec03a49"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No GPU available, using the CPU instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create the dataframes\n",
        "\n",
        "\n",
        "label_mapping = {}\n",
        "with open(\"tweeteval/datasets/emotion/mapping.txt\", 'r') as file:\n",
        "    for line in file:\n",
        "        label, emotion = line.strip().split('\\t')\n",
        "        label_mapping[label] = emotion\n",
        "\n",
        "\n",
        "# Load and process your data\n",
        "def load_and_filter_data(text_file, label_file):\n",
        "    texts = []\n",
        "    labels = []\n",
        "    with open(text_file, 'r') as file1, open(label_file, 'r') as file2:\n",
        "        while True:\n",
        "            text = file1.readline().strip()\n",
        "            label = file2.readline().strip()\n",
        "\n",
        "            if not text or not label:\n",
        "                break\n",
        "\n",
        "            emotion = label_mapping.get(label, label)  # Map label to emotion using the mapping\n",
        "            texts.append(text)\n",
        "            labels.append(emotion)\n",
        "\n",
        "    return texts, labels\n",
        "\n",
        "\n",
        "train_df = pd.DataFrame()\n",
        "val_df = pd.DataFrame()\n",
        "test_df = pd.DataFrame()\n",
        "train_df[\"text\"], train_df[\"label\"] = load_and_filter_data(\"tweeteval/datasets/emotion/train_text.txt\", \"tweeteval/datasets/emotion/train_labels.txt\")\n",
        "val_df[\"text\"], val_df[\"label\"] = load_and_filter_data(\"tweeteval/datasets/emotion/val_text.txt\", \"tweeteval/datasets/emotion/val_labels.txt\")\n",
        "test_df[\"text\"], test_df[\"label\"] = load_and_filter_data(\"tweeteval/datasets/emotion/test_text.txt\", \"tweeteval/datasets/emotion/test_labels.txt\")\n",
        "train_df.drop_duplicates(inplace=True)\n",
        "\n",
        "X_train, y_train = train_df[\"text\"], train_df[\"label\"]\n",
        "X_val, y_val= val_df[\"text\"], val_df[\"label\"]\n",
        "X_test, y_test = test_df[\"text\"], test_df[\"label\"]"
      ],
      "metadata": {
        "id": "8FbKXtz5wXzn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#preprocess imports\n",
        "import nltk\n",
        "import string\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import TweetTokenizer, sent_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "import string\n"
      ],
      "metadata": {
        "id": "-ddL425WyQnd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be023464-6a17-4543-fba8-d925a59f37c3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing(tweet_text):\n",
        "    tweet_text = tweet_text.lower()\n",
        "    tokenizer = TweetTokenizer()\n",
        "    tokens = tokenizer.tokenize(tweet_text)\n",
        "  # Remove stopwords, punctuation, hashtags, mentions, URLs\n",
        "    tokens = [token for token in tokens if token not in stopwords.words('english') and token not in string.punctuation and not token.startswith(\"#\") and not token.startswith(\"@\") and not token.startswith(\"http\")]\n",
        "\n",
        "#emojis\n",
        "    tokens = [token for token in tokens if token.isalnum() or token.isascii()]\n",
        "\n",
        "    processed_tweet = ' '.join(tokens)\n",
        "    return processed_tweet\n",
        "\n",
        "\n",
        "X_train = X_train.apply(preprocessing)\n",
        "X_val = X_val.apply(preprocessing)\n",
        "X_test = X_test.apply(preprocessing)\n"
      ],
      "metadata": {
        "id": "gIlQcDBz0Tsy"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenize, vectorize, and tensor\n",
        "vocab = set(X_train)\n",
        "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
        "ix_to_word = {i: word for i, word in enumerate(vocab)}\n",
        "\n",
        "word_to_ix['<unk>'] = len(word_to_ix)\n",
        "ix_to_word[len(word_to_ix)] = '<unk>'\n",
        "\n",
        "def tokenize_and_index(text, word_to_ix):\n",
        "    tokens = text.split()\n",
        "    indexed_tokens = [word_to_ix.get(token, word_to_ix['<unk>']) for token in tokens]\n",
        "    return indexed_tokens\n",
        "\n",
        "# Apply tokenization and indexing\n",
        "X_train_indices = [tokenize_and_index(text, word_to_ix) for text in X_train]\n",
        "X_val_indices = [tokenize_and_index(text, word_to_ix) for text in X_val]\n",
        "X_test_indices = [tokenize_and_index(text, word_to_ix) for text in X_test]\n",
        "\n",
        "\n",
        "\n",
        "X_train_tensor = [torch.LongTensor(indices) for indices in X_train_indices]\n",
        "X_val_tensor = [torch.LongTensor(indices) for indices in X_val_indices]\n",
        "X_test_tensor = [torch.LongTensor(indices) for indices in X_test_indices]\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "X_train_tensor = pad_sequence([torch.LongTensor(indices) for indices in X_train_indices], batch_first=True)\n",
        "X_val_tensor = pad_sequence([torch.LongTensor(indices) for indices in X_val_indices], batch_first=True)\n",
        "X_test_tensor = pad_sequence([torch.LongTensor(indices) for indices in X_test_indices], batch_first=True)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VT4VZXf1Me9i"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_train.value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gt3CXbKH9Uv1",
        "outputId": "fb4f074e-88db-4562-8422-faf449796084"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "anger       1400\n",
            "sadness      855\n",
            "joy          683\n",
            "optimism     294\n",
            "Name: label, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# manual labeling and encoding\n",
        "label_mapping = {'anger': 0, 'sadness': 1, 'joy':2, 'optimism':3}\n",
        "\n",
        "y_train_encoded = [label_mapping[label] for label in y_train]\n",
        "y_val_encoded = [label_mapping[label] for label in y_val]\n",
        "y_test_encoded = [label_mapping[label] for label in y_test]\n",
        "\n",
        "y_train_tensor = torch.Tensor(y_train_encoded).long()\n",
        "y_val_tensor = torch.Tensor(y_val_encoded).long()\n",
        "y_test_tensor = torch.Tensor(y_test_encoded).long()"
      ],
      "metadata": {
        "id": "XVksWG769-1D"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DataLoader for sets\n",
        "batch_size = 128\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "B-wc9avSNvTN"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CNN structure\n",
        "\n",
        "class SentimentCNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, filter_sizes, num_filters,  output_dim, dropout):\n",
        "        super(SentimentCNN, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.convs = nn.ModuleList([\n",
        "            nn.Conv2d(in_channels=1, out_channels=nf, kernel_size=(fs, embedding_dim))\n",
        "            for nf, fs in zip(num_filters, filter_sizes)\n",
        "        ])\n",
        "        self.fc = nn.Linear(len(filter_sizes)*num_filters[0], output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text):\n",
        "        embedded = self.embedding(text)\n",
        "        embedded = embedded.unsqueeze(1)\n",
        "\n",
        "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
        "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
        "\n",
        "        cat = self.dropout(torch.cat(pooled, dim=1))\n",
        "\n",
        "        return self.fc(cat)\n",
        "\n"
      ],
      "metadata": {
        "id": "JzCPDJob0Y31"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, classification_report"
      ],
      "metadata": {
        "id": "0NVAKWEG2Aqv"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get the best number of filters\n",
        "\n",
        "import itertools\n",
        "numbers = [3, 4, 5]\n",
        "num_filters = [[150, 150,150], [200,200,200], [250,250,250]]\n",
        "filter_sizes = [3,4,5]\n",
        "# Initialize variables to keep track of the best hyperparameters and metrics\n",
        "best_hyperparameters = None\n",
        "best_accuracy = 0.0\n",
        "\n",
        "# Perform grid search\n",
        "for filter_size, num_filter,  in itertools.product(filter_sizes, num_filters):\n",
        "    batch_size = 128\n",
        "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "    # Create a new model with the current hyperparameters\n",
        "    model = SentimentCNN(\n",
        "        vocab_size=len(word_to_ix),\n",
        "        embedding_dim=500,\n",
        "        filter_sizes=filter_size,\n",
        "        num_filters=num_filter,\n",
        "        output_dim=4,\n",
        "        dropout=0.2\n",
        "    )\n",
        "\n",
        "    # Define the loss and optimizer\n",
        "    criterion = nn.CrossEntropyLoss(weight = torch.FloatTensor([1.0, 2.0, 2.0, 6.0]))\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    num_epochs = 10  # Adjust the number of epochs as needed\n",
        "\n",
        "    # Training loop (similar to your existing code)\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "      model.train()\n",
        "      total_loss = 0.0\n",
        "\n",
        "      for batch in train_loader:\n",
        "          inputs, labels = batch\n",
        "          optimizer.zero_grad()\n",
        "          outputs = model(inputs)\n",
        "          loss = criterion(outputs, labels)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          total_loss += loss.item()\n",
        "\n",
        "      # Calculate and print the average loss for this epoch\n",
        "      avg_loss = total_loss / len(train_loader)\n",
        "      print(f'Epoch [{epoch+1}/{num_epochs}] - Train Loss: {avg_loss:.4f}')\n",
        "\n",
        "      # Validation\n",
        "      model.eval() #removes dropout\n",
        "      val_loss = 0.0\n",
        "      correct = 0\n",
        "      total = 0\n",
        "      all_predictions = []\n",
        "      with torch.no_grad():\n",
        "          for batch in val_loader:\n",
        "              inputs, labels = batch\n",
        "              outputs = model(inputs)\n",
        "              loss = criterion(outputs, labels)\n",
        "              val_loss += loss.item()\n",
        "              _, predicted = torch.max(outputs.data, 1)\n",
        "              total += labels.size(0)\n",
        "              correct += (predicted == labels).sum().item()\n",
        "              all_predictions.extend(predicted.tolist())\n",
        "      val_accuracy = 100 * correct / total\n",
        "      val_loss /= len(val_loader)\n",
        "\n",
        "      print(f'Epoch [{epoch+1}/{num_epochs}] - Validation Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.2f}%')\n",
        "\n",
        "\n",
        "    # Calculate accuracy and F1-macro scores\n",
        "    accuracy = accuracy_score(y_val_encoded, all_predictions)\n",
        "\n",
        "    # Keep track of the best hyperparameters\n",
        "    if accuracy > best_accuracy:\n",
        "        best_accuracy = accuracy\n",
        "        best_hyperparameters = (num_filter, filter_size)\n",
        "\n",
        "# Print the best hyperparameters and their corresponding accuracy\n",
        "print(\"Best Hyperparameters:\")\n",
        "print(\"Number of filters:\", best_hyperparameters[0])\n",
        "print(\"Filter size:\", best_hyperparameters[1])\n",
        "print(\"Best Accuracy:\", best_accuracy)"
      ],
      "metadata": {
        "id": "BSoxBUc0DIrw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "534e067b-4e5d-43c8-c131-9777846d7b7f"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([32, 450])\n",
            "Epoch [1/10] - Train Loss: 1.8774\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([118, 450])\n",
            "Epoch [1/10] - Validation Loss: 1.3835, Accuracy: 15.78%\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([32, 450])\n",
            "Epoch [2/10] - Train Loss: 1.4057\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([118, 450])\n",
            "Epoch [2/10] - Validation Loss: 1.3700, Accuracy: 42.25%\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([32, 450])\n",
            "Epoch [3/10] - Train Loss: 1.3494\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([118, 450])\n",
            "Epoch [3/10] - Validation Loss: 1.3399, Accuracy: 25.67%\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([32, 450])\n",
            "Epoch [4/10] - Train Loss: 1.3178\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([118, 450])\n",
            "Epoch [4/10] - Validation Loss: 1.3434, Accuracy: 20.32%\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([32, 450])\n",
            "Epoch [5/10] - Train Loss: 1.2768\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([118, 450])\n",
            "Epoch [5/10] - Validation Loss: 1.3468, Accuracy: 21.66%\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([32, 450])\n",
            "Epoch [6/10] - Train Loss: 1.2650\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([118, 450])\n",
            "Epoch [6/10] - Validation Loss: 1.3063, Accuracy: 25.13%\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([32, 450])\n",
            "Epoch [7/10] - Train Loss: 1.2495\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([118, 450])\n",
            "Epoch [7/10] - Validation Loss: 1.3171, Accuracy: 21.66%\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([32, 450])\n",
            "Epoch [8/10] - Train Loss: 1.2560\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([118, 450])\n",
            "Epoch [8/10] - Validation Loss: 1.3368, Accuracy: 24.33%\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([32, 450])\n",
            "Epoch [9/10] - Train Loss: 1.2479\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([118, 450])\n",
            "Epoch [9/10] - Validation Loss: 1.3275, Accuracy: 41.71%\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([32, 450])\n",
            "Epoch [10/10] - Train Loss: 1.2454\n",
            "torch.Size([128, 450])\n",
            "torch.Size([128, 450])\n",
            "torch.Size([118, 450])\n",
            "Epoch [10/10] - Validation Loss: 1.3159, Accuracy: 22.19%\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([32, 600])\n",
            "Epoch [1/10] - Train Loss: 1.9755\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([118, 600])\n",
            "Epoch [1/10] - Validation Loss: 1.6703, Accuracy: 23.80%\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([32, 600])\n",
            "Epoch [2/10] - Train Loss: 1.4242\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([118, 600])\n",
            "Epoch [2/10] - Validation Loss: 1.3358, Accuracy: 29.14%\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([32, 600])\n",
            "Epoch [3/10] - Train Loss: 1.3520\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([118, 600])\n",
            "Epoch [3/10] - Validation Loss: 1.3284, Accuracy: 22.73%\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([32, 600])\n",
            "Epoch [4/10] - Train Loss: 1.3209\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([118, 600])\n",
            "Epoch [4/10] - Validation Loss: 1.3118, Accuracy: 28.88%\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([32, 600])\n",
            "Epoch [5/10] - Train Loss: 1.2897\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([118, 600])\n",
            "Epoch [5/10] - Validation Loss: 1.3437, Accuracy: 22.73%\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([32, 600])\n",
            "Epoch [6/10] - Train Loss: 1.2791\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([118, 600])\n",
            "Epoch [6/10] - Validation Loss: 1.3300, Accuracy: 21.12%\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([32, 600])\n",
            "Epoch [7/10] - Train Loss: 1.2650\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([118, 600])\n",
            "Epoch [7/10] - Validation Loss: 1.3140, Accuracy: 23.80%\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([32, 600])\n",
            "Epoch [8/10] - Train Loss: 1.2464\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([118, 600])\n",
            "Epoch [8/10] - Validation Loss: 1.2972, Accuracy: 25.13%\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([32, 600])\n",
            "Epoch [9/10] - Train Loss: 1.2477\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([118, 600])\n",
            "Epoch [9/10] - Validation Loss: 1.2950, Accuracy: 29.14%\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([32, 600])\n",
            "Epoch [10/10] - Train Loss: 1.2297\n",
            "torch.Size([128, 600])\n",
            "torch.Size([128, 600])\n",
            "torch.Size([118, 600])\n",
            "Epoch [10/10] - Validation Loss: 1.3126, Accuracy: 24.87%\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([32, 750])\n",
            "Epoch [1/10] - Train Loss: 2.1264\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([118, 750])\n",
            "Epoch [1/10] - Validation Loss: 1.4742, Accuracy: 44.39%\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([32, 750])\n",
            "Epoch [2/10] - Train Loss: 1.4008\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([118, 750])\n",
            "Epoch [2/10] - Validation Loss: 1.3502, Accuracy: 40.64%\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([32, 750])\n",
            "Epoch [3/10] - Train Loss: 1.3350\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([118, 750])\n",
            "Epoch [3/10] - Validation Loss: 1.3322, Accuracy: 24.06%\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([32, 750])\n",
            "Epoch [4/10] - Train Loss: 1.3044\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([118, 750])\n",
            "Epoch [4/10] - Validation Loss: 1.3480, Accuracy: 45.99%\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([32, 750])\n",
            "Epoch [5/10] - Train Loss: 1.2996\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([118, 750])\n",
            "Epoch [5/10] - Validation Loss: 1.3235, Accuracy: 41.71%\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([32, 750])\n",
            "Epoch [6/10] - Train Loss: 1.2832\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([118, 750])\n",
            "Epoch [6/10] - Validation Loss: 1.3203, Accuracy: 28.34%\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([32, 750])\n",
            "Epoch [7/10] - Train Loss: 1.2659\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([118, 750])\n",
            "Epoch [7/10] - Validation Loss: 1.3053, Accuracy: 27.54%\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([32, 750])\n",
            "Epoch [8/10] - Train Loss: 1.2518\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([118, 750])\n",
            "Epoch [8/10] - Validation Loss: 1.3398, Accuracy: 20.59%\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([32, 750])\n",
            "Epoch [9/10] - Train Loss: 1.2416\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([118, 750])\n",
            "Epoch [9/10] - Validation Loss: 1.3386, Accuracy: 18.98%\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([32, 750])\n",
            "Epoch [10/10] - Train Loss: 1.2390\n",
            "torch.Size([128, 750])\n",
            "torch.Size([128, 750])\n",
            "torch.Size([118, 750])\n",
            "Epoch [10/10] - Validation Loss: 1.3219, Accuracy: 22.73%\n",
            "Best Hyperparameters:\n",
            "Number of filters: [200, 200, 200]\n",
            "Filter size: [3, 4, 5]\n",
            "Best Accuracy: 0.24866310160427807\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#get the best filter size\n",
        "numbers = [3, 4, 5]\n",
        "filter_sizes = [list(combination) for combination in itertools.product(numbers, repeat=3)]\n",
        "# Initialize variables to keep track of the best hyperparameters and metrics\n",
        "best_hyperparameters = None\n",
        "best_accuracy = 0.0\n",
        "\n",
        "# Perform grid search\n",
        "for filter_size, in itertools.product(filter_sizes):\n",
        "    batch_size = 128\n",
        "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "    # Create a new model with the current hyperparameters\n",
        "    model = SentimentCNN(\n",
        "        vocab_size=len(word_to_ix),\n",
        "        embedding_dim=500,\n",
        "        filter_sizes=filter_size,\n",
        "        num_filters=[200,200,200],\n",
        "        output_dim=4,\n",
        "        dropout=0.2\n",
        "    )\n",
        "\n",
        "    # Define the loss and optimizer\n",
        "    criterion = nn.CrossEntropyLoss(weight = torch.FloatTensor([1.0, 2.0, 2.0, 6.0]))\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    num_epochs = 10  # Adjust the number of epochs as needed\n",
        "\n",
        "    # Training loop (similar to your existing code)\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "      model.train()\n",
        "      total_loss = 0.0\n",
        "\n",
        "      for batch in train_loader:\n",
        "          inputs, labels = batch\n",
        "          optimizer.zero_grad()\n",
        "          outputs = model(inputs)\n",
        "          loss = criterion(outputs, labels)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          total_loss += loss.item()\n",
        "\n",
        "      # Calculate and print the average loss for this epoch\n",
        "      avg_loss = total_loss / len(train_loader)\n",
        "      print(f'Epoch [{epoch+1}/{num_epochs}] - Train Loss: {avg_loss:.4f}')\n",
        "\n",
        "      # Validation\n",
        "      model.eval() #removes dropout\n",
        "      val_loss = 0.0\n",
        "      correct = 0\n",
        "      total = 0\n",
        "      all_predictions = []\n",
        "      with torch.no_grad():\n",
        "          for batch in val_loader:\n",
        "              inputs, labels = batch\n",
        "              outputs = model(inputs)\n",
        "              loss = criterion(outputs, labels)\n",
        "              val_loss += loss.item()\n",
        "              _, predicted = torch.max(outputs.data, 1)\n",
        "              total += labels.size(0)\n",
        "              correct += (predicted == labels).sum().item()\n",
        "              all_predictions.extend(predicted.tolist())\n",
        "      val_accuracy = 100 * correct / total\n",
        "      val_loss /= len(val_loader)\n",
        "\n",
        "      print(f'Epoch [{epoch+1}/{num_epochs}] - Validation Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.2f}%')\n",
        "\n",
        "\n",
        "    # Calculate accuracy and F1-macro scores\n",
        "    accuracy = accuracy_score(y_val_encoded, all_predictions)\n",
        "\n",
        "    # Keep track of the best hyperparameters\n",
        "    if accuracy > best_accuracy:\n",
        "        best_accuracy = accuracy\n",
        "        best_hyperparameters = (filter_size)\n",
        "\n",
        "# Print the best hyperparameters and their corresponding accuracy\n",
        "print(\"Best Hyperparameters:\")\n",
        "print(\"Filter size:\", best_hyperparameters[0])\n",
        "print(\"Best Accuracy:\", best_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OuCsPVaET5ST",
        "outputId": "c084e4c0-617b-44a2-df21-5c17a0c5aeff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10] - Train Loss: 1.8170\n",
            "Epoch [1/10] - Validation Loss: 1.3673, Accuracy: 20.32%\n",
            "Epoch [2/10] - Train Loss: 1.3670\n",
            "Epoch [2/10] - Validation Loss: 1.4447, Accuracy: 15.78%\n",
            "Epoch [3/10] - Train Loss: 1.3434\n",
            "Epoch [3/10] - Validation Loss: 1.3651, Accuracy: 19.52%\n",
            "Epoch [4/10] - Train Loss: 1.3066\n",
            "Epoch [4/10] - Validation Loss: 1.3427, Accuracy: 28.61%\n",
            "Epoch [5/10] - Train Loss: 1.2816\n",
            "Epoch [5/10] - Validation Loss: 1.3350, Accuracy: 21.12%\n",
            "Epoch [6/10] - Train Loss: 1.2609\n",
            "Epoch [6/10] - Validation Loss: 1.3199, Accuracy: 22.73%\n",
            "Epoch [7/10] - Train Loss: 1.2494\n",
            "Epoch [7/10] - Validation Loss: 1.3198, Accuracy: 30.48%\n",
            "Epoch [8/10] - Train Loss: 1.2401\n",
            "Epoch [8/10] - Validation Loss: 1.3285, Accuracy: 25.67%\n",
            "Epoch [9/10] - Train Loss: 1.2388\n",
            "Epoch [9/10] - Validation Loss: 1.3051, Accuracy: 25.13%\n",
            "Epoch [10/10] - Train Loss: 1.2438\n",
            "Epoch [10/10] - Validation Loss: 1.3048, Accuracy: 27.01%\n",
            "Epoch [1/10] - Train Loss: 2.0329\n",
            "Epoch [1/10] - Validation Loss: 1.4254, Accuracy: 24.06%\n",
            "Epoch [2/10] - Train Loss: 1.4126\n",
            "Epoch [2/10] - Validation Loss: 1.3611, Accuracy: 41.44%\n",
            "Epoch [3/10] - Train Loss: 1.3481\n",
            "Epoch [3/10] - Validation Loss: 1.3416, Accuracy: 27.81%\n",
            "Epoch [4/10] - Train Loss: 1.2924\n",
            "Epoch [4/10] - Validation Loss: 1.3298, Accuracy: 20.32%\n",
            "Epoch [5/10] - Train Loss: 1.2859\n",
            "Epoch [5/10] - Validation Loss: 1.3051, Accuracy: 32.62%\n",
            "Epoch [6/10] - Train Loss: 1.2684\n",
            "Epoch [6/10] - Validation Loss: 1.3290, Accuracy: 20.32%\n",
            "Epoch [7/10] - Train Loss: 1.2560\n",
            "Epoch [7/10] - Validation Loss: 1.3279, Accuracy: 22.19%\n",
            "Epoch [8/10] - Train Loss: 1.2524\n",
            "Epoch [8/10] - Validation Loss: 1.3117, Accuracy: 26.47%\n",
            "Epoch [9/10] - Train Loss: 1.2466\n",
            "Epoch [9/10] - Validation Loss: 1.3633, Accuracy: 20.59%\n",
            "Epoch [10/10] - Train Loss: 1.2397\n",
            "Epoch [10/10] - Validation Loss: 1.3382, Accuracy: 18.98%\n",
            "Epoch [1/10] - Train Loss: 2.0440\n",
            "Epoch [1/10] - Validation Loss: 1.3993, Accuracy: 18.98%\n",
            "Epoch [2/10] - Train Loss: 1.3856\n",
            "Epoch [2/10] - Validation Loss: 1.3532, Accuracy: 19.25%\n",
            "Epoch [3/10] - Train Loss: 1.3462\n",
            "Epoch [3/10] - Validation Loss: 1.3625, Accuracy: 18.98%\n",
            "Epoch [4/10] - Train Loss: 1.3083\n",
            "Epoch [4/10] - Validation Loss: 1.3227, Accuracy: 24.87%\n",
            "Epoch [5/10] - Train Loss: 1.3071\n",
            "Epoch [5/10] - Validation Loss: 1.3092, Accuracy: 45.45%\n",
            "Epoch [6/10] - Train Loss: 1.2651\n",
            "Epoch [6/10] - Validation Loss: 1.3072, Accuracy: 47.33%\n",
            "Epoch [7/10] - Train Loss: 1.2499\n",
            "Epoch [7/10] - Validation Loss: 1.2957, Accuracy: 26.20%\n",
            "Epoch [8/10] - Train Loss: 1.2439\n",
            "Epoch [8/10] - Validation Loss: 1.2912, Accuracy: 24.60%\n",
            "Epoch [9/10] - Train Loss: 1.2324\n",
            "Epoch [9/10] - Validation Loss: 1.3055, Accuracy: 44.92%\n",
            "Epoch [10/10] - Train Loss: 1.2441\n",
            "Epoch [10/10] - Validation Loss: 1.3033, Accuracy: 48.93%\n",
            "Epoch [1/10] - Train Loss: 2.0093\n",
            "Epoch [1/10] - Validation Loss: 1.4572, Accuracy: 13.37%\n",
            "Epoch [2/10] - Train Loss: 1.3931\n",
            "Epoch [2/10] - Validation Loss: 1.3788, Accuracy: 20.05%\n",
            "Epoch [3/10] - Train Loss: 1.3182\n",
            "Epoch [3/10] - Validation Loss: 1.3539, Accuracy: 21.66%\n",
            "Epoch [4/10] - Train Loss: 1.3055\n",
            "Epoch [4/10] - Validation Loss: 1.3404, Accuracy: 22.73%\n",
            "Epoch [5/10] - Train Loss: 1.2843\n",
            "Epoch [5/10] - Validation Loss: 1.2959, Accuracy: 28.61%\n",
            "Epoch [6/10] - Train Loss: 1.2900\n",
            "Epoch [6/10] - Validation Loss: 1.3209, Accuracy: 29.41%\n",
            "Epoch [7/10] - Train Loss: 1.2618\n",
            "Epoch [7/10] - Validation Loss: 1.3021, Accuracy: 25.67%\n",
            "Epoch [8/10] - Train Loss: 1.2556\n",
            "Epoch [8/10] - Validation Loss: 1.3040, Accuracy: 33.16%\n",
            "Epoch [9/10] - Train Loss: 1.2412\n",
            "Epoch [9/10] - Validation Loss: 1.3210, Accuracy: 30.21%\n",
            "Epoch [10/10] - Train Loss: 1.2376\n",
            "Epoch [10/10] - Validation Loss: 1.3321, Accuracy: 24.06%\n",
            "Epoch [1/10] - Train Loss: 1.9603\n",
            "Epoch [1/10] - Validation Loss: 1.5483, Accuracy: 44.39%\n",
            "Epoch [2/10] - Train Loss: 1.4099\n",
            "Epoch [2/10] - Validation Loss: 1.3928, Accuracy: 17.38%\n",
            "Epoch [3/10] - Train Loss: 1.3462\n",
            "Epoch [3/10] - Validation Loss: 1.3509, Accuracy: 26.20%\n",
            "Epoch [4/10] - Train Loss: 1.3065\n",
            "Epoch [4/10] - Validation Loss: 1.3450, Accuracy: 22.46%\n",
            "Epoch [5/10] - Train Loss: 1.2981\n",
            "Epoch [5/10] - Validation Loss: 1.3806, Accuracy: 20.05%\n",
            "Epoch [6/10] - Train Loss: 1.2794\n",
            "Epoch [6/10] - Validation Loss: 1.3111, Accuracy: 28.61%\n",
            "Epoch [7/10] - Train Loss: 1.2669\n",
            "Epoch [7/10] - Validation Loss: 1.3278, Accuracy: 28.88%\n",
            "Epoch [8/10] - Train Loss: 1.2510\n",
            "Epoch [8/10] - Validation Loss: 1.3412, Accuracy: 28.34%\n",
            "Epoch [9/10] - Train Loss: 1.2529\n",
            "Epoch [9/10] - Validation Loss: 1.3022, Accuracy: 46.79%\n",
            "Epoch [10/10] - Train Loss: 1.2458\n",
            "Epoch [10/10] - Validation Loss: 1.3132, Accuracy: 29.68%\n",
            "Epoch [1/10] - Train Loss: 1.8587\n",
            "Epoch [1/10] - Validation Loss: 1.3747, Accuracy: 23.80%\n",
            "Epoch [2/10] - Train Loss: 1.3732\n",
            "Epoch [2/10] - Validation Loss: 1.3759, Accuracy: 21.39%\n",
            "Epoch [3/10] - Train Loss: 1.3145\n",
            "Epoch [3/10] - Validation Loss: 1.3369, Accuracy: 24.33%\n",
            "Epoch [4/10] - Train Loss: 1.2899\n",
            "Epoch [4/10] - Validation Loss: 1.3178, Accuracy: 27.54%\n",
            "Epoch [5/10] - Train Loss: 1.2884\n",
            "Epoch [5/10] - Validation Loss: 1.3278, Accuracy: 46.26%\n",
            "Epoch [6/10] - Train Loss: 1.2557\n",
            "Epoch [6/10] - Validation Loss: 1.3104, Accuracy: 29.41%\n",
            "Epoch [7/10] - Train Loss: 1.2530\n",
            "Epoch [7/10] - Validation Loss: 1.3029, Accuracy: 24.60%\n",
            "Epoch [8/10] - Train Loss: 1.2399\n",
            "Epoch [8/10] - Validation Loss: 1.3169, Accuracy: 27.54%\n",
            "Epoch [9/10] - Train Loss: 1.2426\n",
            "Epoch [9/10] - Validation Loss: 1.3072, Accuracy: 27.01%\n",
            "Epoch [10/10] - Train Loss: 1.2328\n",
            "Epoch [10/10] - Validation Loss: 1.3389, Accuracy: 20.59%\n",
            "Epoch [1/10] - Train Loss: 2.0753\n",
            "Epoch [1/10] - Validation Loss: 1.5399, Accuracy: 40.11%\n",
            "Epoch [2/10] - Train Loss: 1.3940\n",
            "Epoch [2/10] - Validation Loss: 1.3302, Accuracy: 22.99%\n",
            "Epoch [3/10] - Train Loss: 1.3411\n",
            "Epoch [3/10] - Validation Loss: 1.3663, Accuracy: 24.06%\n",
            "Epoch [4/10] - Train Loss: 1.3033\n",
            "Epoch [4/10] - Validation Loss: 1.3323, Accuracy: 23.80%\n",
            "Epoch [5/10] - Train Loss: 1.2835\n",
            "Epoch [5/10] - Validation Loss: 1.3712, Accuracy: 21.39%\n",
            "Epoch [6/10] - Train Loss: 1.2820\n",
            "Epoch [6/10] - Validation Loss: 1.3754, Accuracy: 27.01%\n",
            "Epoch [7/10] - Train Loss: 1.2689\n",
            "Epoch [7/10] - Validation Loss: 1.3044, Accuracy: 47.86%\n",
            "Epoch [8/10] - Train Loss: 1.2428\n",
            "Epoch [8/10] - Validation Loss: 1.3336, Accuracy: 22.73%\n",
            "Epoch [9/10] - Train Loss: 1.2406\n",
            "Epoch [9/10] - Validation Loss: 1.3145, Accuracy: 24.06%\n",
            "Epoch [10/10] - Train Loss: 1.2391\n",
            "Epoch [10/10] - Validation Loss: 1.3038, Accuracy: 24.33%\n",
            "Epoch [1/10] - Train Loss: 1.9515\n",
            "Epoch [1/10] - Validation Loss: 1.4157, Accuracy: 26.20%\n",
            "Epoch [2/10] - Train Loss: 1.4284\n",
            "Epoch [2/10] - Validation Loss: 1.4226, Accuracy: 13.10%\n",
            "Epoch [3/10] - Train Loss: 1.3395\n",
            "Epoch [3/10] - Validation Loss: 1.3512, Accuracy: 28.07%\n",
            "Epoch [4/10] - Train Loss: 1.3415\n",
            "Epoch [4/10] - Validation Loss: 1.3362, Accuracy: 22.46%\n",
            "Epoch [5/10] - Train Loss: 1.2959\n",
            "Epoch [5/10] - Validation Loss: 1.3267, Accuracy: 45.45%\n",
            "Epoch [6/10] - Train Loss: 1.2862\n",
            "Epoch [6/10] - Validation Loss: 1.3528, Accuracy: 28.88%\n",
            "Epoch [7/10] - Train Loss: 1.2531\n",
            "Epoch [7/10] - Validation Loss: 1.3587, Accuracy: 21.12%\n",
            "Epoch [8/10] - Train Loss: 1.2574\n",
            "Epoch [8/10] - Validation Loss: 1.3393, Accuracy: 22.99%\n",
            "Epoch [9/10] - Train Loss: 1.2472\n",
            "Epoch [9/10] - Validation Loss: 1.3168, Accuracy: 23.80%\n",
            "Epoch [10/10] - Train Loss: 1.2292\n",
            "Epoch [10/10] - Validation Loss: 1.3027, Accuracy: 29.14%\n",
            "Epoch [1/10] - Train Loss: 1.8885\n",
            "Epoch [1/10] - Validation Loss: 1.3579, Accuracy: 21.39%\n",
            "Epoch [2/10] - Train Loss: 1.3739\n",
            "Epoch [2/10] - Validation Loss: 1.3794, Accuracy: 24.87%\n",
            "Epoch [3/10] - Train Loss: 1.3585\n",
            "Epoch [3/10] - Validation Loss: 1.3262, Accuracy: 32.62%\n",
            "Epoch [4/10] - Train Loss: 1.3118\n",
            "Epoch [4/10] - Validation Loss: 1.3238, Accuracy: 22.73%\n",
            "Epoch [5/10] - Train Loss: 1.2972\n",
            "Epoch [5/10] - Validation Loss: 1.3050, Accuracy: 27.01%\n",
            "Epoch [6/10] - Train Loss: 1.2737\n",
            "Epoch [6/10] - Validation Loss: 1.3419, Accuracy: 19.79%\n",
            "Epoch [7/10] - Train Loss: 1.2669\n",
            "Epoch [7/10] - Validation Loss: 1.3405, Accuracy: 28.07%\n",
            "Epoch [8/10] - Train Loss: 1.2591\n",
            "Epoch [8/10] - Validation Loss: 1.3123, Accuracy: 24.60%\n",
            "Epoch [9/10] - Train Loss: 1.2442\n",
            "Epoch [9/10] - Validation Loss: 1.3231, Accuracy: 22.19%\n",
            "Epoch [10/10] - Train Loss: 1.2409\n",
            "Epoch [10/10] - Validation Loss: 1.3476, Accuracy: 28.61%\n",
            "Epoch [1/10] - Train Loss: 1.6598\n",
            "Epoch [1/10] - Validation Loss: 1.4382, Accuracy: 48.40%\n",
            "Epoch [2/10] - Train Loss: 1.3873\n",
            "Epoch [2/10] - Validation Loss: 1.3482, Accuracy: 31.28%\n",
            "Epoch [3/10] - Train Loss: 1.3791\n",
            "Epoch [3/10] - Validation Loss: 1.3447, Accuracy: 20.59%\n",
            "Epoch [4/10] - Train Loss: 1.3249\n",
            "Epoch [4/10] - Validation Loss: 1.3122, Accuracy: 46.26%\n",
            "Epoch [5/10] - Train Loss: 1.2752\n",
            "Epoch [5/10] - Validation Loss: 1.3287, Accuracy: 45.45%\n",
            "Epoch [6/10] - Train Loss: 1.2697\n",
            "Epoch [6/10] - Validation Loss: 1.3090, Accuracy: 24.33%\n",
            "Epoch [7/10] - Train Loss: 1.2523\n",
            "Epoch [7/10] - Validation Loss: 1.2989, Accuracy: 46.52%\n",
            "Epoch [8/10] - Train Loss: 1.2438\n",
            "Epoch [8/10] - Validation Loss: 1.3455, Accuracy: 25.13%\n",
            "Epoch [9/10] - Train Loss: 1.2414\n",
            "Epoch [9/10] - Validation Loss: 1.3043, Accuracy: 30.21%\n",
            "Epoch [10/10] - Train Loss: 1.2199\n",
            "Epoch [10/10] - Validation Loss: 1.3617, Accuracy: 28.07%\n",
            "Epoch [1/10] - Train Loss: 2.1573\n",
            "Epoch [1/10] - Validation Loss: 1.4090, Accuracy: 16.84%\n",
            "Epoch [2/10] - Train Loss: 1.4442\n",
            "Epoch [2/10] - Validation Loss: 1.4760, Accuracy: 12.83%\n",
            "Epoch [3/10] - Train Loss: 1.3444\n",
            "Epoch [3/10] - Validation Loss: 1.3552, Accuracy: 46.52%\n",
            "Epoch [4/10] - Train Loss: 1.3185\n",
            "Epoch [4/10] - Validation Loss: 1.3231, Accuracy: 22.46%\n",
            "Epoch [5/10] - Train Loss: 1.3077\n",
            "Epoch [5/10] - Validation Loss: 1.3289, Accuracy: 47.59%\n",
            "Epoch [6/10] - Train Loss: 1.2722\n",
            "Epoch [6/10] - Validation Loss: 1.3814, Accuracy: 18.72%\n",
            "Epoch [7/10] - Train Loss: 1.2751\n",
            "Epoch [7/10] - Validation Loss: 1.3403, Accuracy: 22.73%\n",
            "Epoch [8/10] - Train Loss: 1.2552\n",
            "Epoch [8/10] - Validation Loss: 1.3073, Accuracy: 24.06%\n",
            "Epoch [9/10] - Train Loss: 1.2374\n",
            "Epoch [9/10] - Validation Loss: 1.3688, Accuracy: 20.86%\n",
            "Epoch [10/10] - Train Loss: 1.2362\n",
            "Epoch [10/10] - Validation Loss: 1.3053, Accuracy: 24.87%\n",
            "Epoch [1/10] - Train Loss: 1.9978\n",
            "Epoch [1/10] - Validation Loss: 1.3621, Accuracy: 22.73%\n",
            "Epoch [2/10] - Train Loss: 1.3958\n",
            "Epoch [2/10] - Validation Loss: 1.4175, Accuracy: 13.64%\n",
            "Epoch [3/10] - Train Loss: 1.3536\n",
            "Epoch [3/10] - Validation Loss: 1.4501, Accuracy: 25.40%\n",
            "Epoch [4/10] - Train Loss: 1.3180\n",
            "Epoch [4/10] - Validation Loss: 1.3540, Accuracy: 22.19%\n",
            "Epoch [5/10] - Train Loss: 1.2856\n",
            "Epoch [5/10] - Validation Loss: 1.2996, Accuracy: 28.61%\n",
            "Epoch [6/10] - Train Loss: 1.2706\n",
            "Epoch [6/10] - Validation Loss: 1.3338, Accuracy: 19.79%\n",
            "Epoch [7/10] - Train Loss: 1.2674\n",
            "Epoch [7/10] - Validation Loss: 1.3292, Accuracy: 24.60%\n",
            "Epoch [8/10] - Train Loss: 1.2460\n",
            "Epoch [8/10] - Validation Loss: 1.3001, Accuracy: 21.93%\n",
            "Epoch [9/10] - Train Loss: 1.2299\n",
            "Epoch [9/10] - Validation Loss: 1.3291, Accuracy: 30.21%\n",
            "Epoch [10/10] - Train Loss: 1.2371\n",
            "Epoch [10/10] - Validation Loss: 1.3132, Accuracy: 46.26%\n",
            "Epoch [1/10] - Train Loss: 2.0884\n",
            "Epoch [1/10] - Validation Loss: 1.4111, Accuracy: 8.29%\n",
            "Epoch [2/10] - Train Loss: 1.3933\n",
            "Epoch [2/10] - Validation Loss: 1.3491, Accuracy: 27.54%\n",
            "Epoch [3/10] - Train Loss: 1.3376\n",
            "Epoch [3/10] - Validation Loss: 1.3608, Accuracy: 21.39%\n",
            "Epoch [4/10] - Train Loss: 1.3095\n",
            "Epoch [4/10] - Validation Loss: 1.3261, Accuracy: 31.55%\n",
            "Epoch [5/10] - Train Loss: 1.2960\n",
            "Epoch [5/10] - Validation Loss: 1.3475, Accuracy: 21.66%\n",
            "Epoch [6/10] - Train Loss: 1.2871\n",
            "Epoch [6/10] - Validation Loss: 1.3241, Accuracy: 23.53%\n",
            "Epoch [7/10] - Train Loss: 1.2664\n",
            "Epoch [7/10] - Validation Loss: 1.3190, Accuracy: 21.93%\n",
            "Epoch [8/10] - Train Loss: 1.2568\n",
            "Epoch [8/10] - Validation Loss: 1.3053, Accuracy: 27.01%\n",
            "Epoch [9/10] - Train Loss: 1.2407\n",
            "Epoch [9/10] - Validation Loss: 1.3280, Accuracy: 28.07%\n",
            "Epoch [10/10] - Train Loss: 1.2533\n",
            "Epoch [10/10] - Validation Loss: 1.2979, Accuracy: 24.60%\n",
            "Epoch [1/10] - Train Loss: 1.7731\n",
            "Epoch [1/10] - Validation Loss: 1.4218, Accuracy: 12.03%\n",
            "Epoch [2/10] - Train Loss: 1.3780\n",
            "Epoch [2/10] - Validation Loss: 1.3450, Accuracy: 31.55%\n",
            "Epoch [3/10] - Train Loss: 1.3730\n",
            "Epoch [3/10] - Validation Loss: 1.3281, Accuracy: 24.06%\n",
            "Epoch [4/10] - Train Loss: 1.3314\n",
            "Epoch [4/10] - Validation Loss: 1.4170, Accuracy: 28.61%\n",
            "Epoch [5/10] - Train Loss: 1.3239\n",
            "Epoch [5/10] - Validation Loss: 1.3395, Accuracy: 33.16%\n",
            "Epoch [6/10] - Train Loss: 1.2931\n",
            "Epoch [6/10] - Validation Loss: 1.3348, Accuracy: 28.34%\n",
            "Epoch [7/10] - Train Loss: 1.2754\n",
            "Epoch [7/10] - Validation Loss: 1.3208, Accuracy: 44.65%\n",
            "Epoch [8/10] - Train Loss: 1.2765\n",
            "Epoch [8/10] - Validation Loss: 1.3863, Accuracy: 29.41%\n",
            "Epoch [9/10] - Train Loss: 1.2725\n",
            "Epoch [9/10] - Validation Loss: 1.3138, Accuracy: 48.13%\n",
            "Epoch [10/10] - Train Loss: 1.2491\n",
            "Epoch [10/10] - Validation Loss: 1.3079, Accuracy: 24.33%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#rerun with the obtained best filters to get the best learning rate, dropout rate, batch size, optimizer, embedding dim, epoch number\n",
        "learning_rates = [0.001, 0.01, 0.0001]\n",
        "dropout_rates = [0.2, 0.5, 0.8]\n",
        "batch_sizes = [128,64]\n",
        "optimizers = [torch.optim.Adam, torch.optim.SGD]\n",
        "embedding_dims = [500,300]\n",
        "epochs = [13, 18]\n",
        "\n",
        "# Initialize variables to keep track of the best hyperparameters and metrics\n",
        "best_hyperparameters = None\n",
        "best_accuracy = 0.0\n",
        "\n",
        "# Perform grid search\n",
        "for learning_rate, dropout_rate, batch_size, optimizer, embedding_dim, epoch  in itertools.product(learning_rates, dropout_rates, batch_sizes, optimizers, embedding_dims, epochs):\n",
        "    batch_size = batch_size\n",
        "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "    # Create a new model with the current hyperparameters\n",
        "    model = SentimentCNN(\n",
        "        vocab_size=len(word_to_ix),\n",
        "        embedding_dim=embedding_dim,\n",
        "        filter_sizes=[,,],\n",
        "        num_filters=[200,200,200],\n",
        "        output_dim=4,\n",
        "        dropout=dropout_rate\n",
        "    )\n",
        "\n",
        "    # Define the loss and optimizer\n",
        "    criterion = nn.CrossEntropyLoss(weight = torch.FloatTensor([1.0, 2.0, 2.0, 6.0]))\n",
        "    optimizer = optimizer(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    num_epochs = epoch  # Adjust the number of epochs as needed\n",
        "\n",
        "    # Training loop (similar to your existing code)\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "      model.train()\n",
        "      total_loss = 0.0\n",
        "\n",
        "      for batch in train_loader:\n",
        "          inputs, labels = batch\n",
        "          optimizer.zero_grad()\n",
        "          outputs = model(inputs)\n",
        "          loss = criterion(outputs, labels)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          total_loss += loss.item()\n",
        "\n",
        "      # Calculate and print the average loss for this epoch\n",
        "      avg_loss = total_loss / len(train_loader)\n",
        "      print(f'Epoch [{epoch+1}/{num_epochs}] - Train Loss: {avg_loss:.4f}')\n",
        "\n",
        "      # Validation\n",
        "      model.eval() #removes dropout\n",
        "      val_loss = 0.0\n",
        "      correct = 0\n",
        "      total = 0\n",
        "      all_predictions = []\n",
        "      with torch.no_grad():\n",
        "          for batch in val_loader:\n",
        "              inputs, labels = batch\n",
        "              outputs = model(inputs)\n",
        "              loss = criterion(outputs, labels)\n",
        "              val_loss += loss.item()\n",
        "              _, predicted = torch.max(outputs.data, 1)\n",
        "              total += labels.size(0)\n",
        "              correct += (predicted == labels).sum().item()\n",
        "              all_predictions.extend(predicted.tolist())\n",
        "      val_accuracy = 100 * correct / total\n",
        "      val_loss /= len(val_loader)\n",
        "\n",
        "      print(f'Epoch [{epoch+1}/{num_epochs}] - Validation Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.2f}%')\n",
        "\n",
        "\n",
        "    # Calculate accuracy and F1-macro scores\n",
        "    accuracy = accuracy_score(y_val_encoded, all_predictions)\n",
        "\n",
        "    # Keep track of the best hyperparameters\n",
        "    if accuracy > best_accuracy:\n",
        "        best_accuracy = accuracy\n",
        "        best_hyperparameters = (learning_rate, dropout_rate, batch_size, optimizer, embedding_dim, epoch)\n",
        "\n",
        "# Print the best hyperparameters and their corresponding accuracy\n",
        "print(\"Best Hyperparameters:\")\n",
        "print(\"Learning Rate:\", best_hyperparameters[0])\n",
        "print(\"Dropout Rate:\", best_hyperparameters[1])\n",
        "print(\"Batch Size:\", best_hyperparameters[2])\n",
        "print(\"Optimizer:\", best_hyperparameters[3])\n",
        "print(\"Embedding Dimensions:\", best_hyperparameters[4])\n",
        "print(\"Epochs:\", best_hyperparameters[5])\n",
        "print(\"Best Accuracy:\", best_accuracy)"
      ],
      "metadata": {
        "id": "T8A0hCGZePtR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# using the best hyperparameters obtained from the iteration above, substitute the hyperparameters for the best model\n",
        "model = SentimentCNN(\n",
        "                     vocab_size=len(word_to_ix),\n",
        "                     embedding_dim= 500,\n",
        "                     filter_sizes=[3, 4, 5],\n",
        "                     num_filters= [300, 350, 400],\n",
        "                     output_dim=4,\n",
        "                     dropout=0.2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(weight = torch.FloatTensor([1.0, 2.0, 2.0, 6.0]))\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
      ],
      "metadata": {
        "id": "172cL-Lt001m"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training and validation loops for the best model\n",
        "\n",
        "\n",
        "num_epochs = 15\n",
        "torch.manual_seed(0)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        inputs, labels = batch\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Calculate and print the average loss for this epoch\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}] - Train Loss: {avg_loss:.4f}')\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    predictionsval = []\n",
        "    labelsval = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            inputs, labels = batch\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            predictionsval.extend(predicted.tolist())\n",
        "            labelsval.extend(labels.tolist())\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    val_accuracy = 100 * correct / total\n",
        "    val_loss /= len(val_loader)\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}] - Validation Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.2f}%')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "-iEY_VonQOo5",
        "outputId": "98eadba6-7115-4f0e-abf5-6c21f1125ad8"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([128, 1050])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-af130edca596>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-cf45a36e6741>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mcat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpooled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (128x1050 and 1200x4)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(correct, \"/\", total)\n",
        "\n",
        "#batch128=172, batch64= 166 batch256=114                            ----> batch128\n",
        "#lr0.001 =172, lr0.01 =76, lr0.0001 = 109                           ----> lr0.001\n",
        "#embed300=172, embed500=180, embed200=108, embed600=109             ---->embed500\n",
        "#drop0.5=177, drop0.2= 180, drop0.8= 102                            ---->drop0.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mITPpw40wMYG",
        "outputId": "e22a0efb-f4ae-4fb2-e78c-99e7d132a346"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "175 / 374\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Calculate accuracy and F1-macro scores for validation set\n",
        "accuracy = accuracy_score(labelsval, predictionsval)\n",
        "f1_macro = f1_score(labelsval, predictionsval, average='macro')\n",
        "\n",
        "# Create a classification report for detailed metrics\n",
        "report = classification_report(labelsval, predictionsval, target_names=['Anger', 'Sadness', 'Joy', 'Optimism'], output_dict=True)\n",
        "\n",
        "# Create a table to display the results\n",
        "results = pd.DataFrame({\n",
        "    'Metric': ['Accuracy', 'F1-Macro'],\n",
        "    'Value': [accuracy, f1_macro]\n",
        "})\n",
        "\n",
        "# Display the table\n",
        "print(results)\n",
        "\n",
        "# Display the detailed classification report\n",
        "print(pd.DataFrame(report).transpose())"
      ],
      "metadata": {
        "id": "pwcBAHGjvhqe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e33fd24-92ba-489d-99a9-014214d28ae5"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Metric     Value\n",
            "0  Accuracy  0.470588\n",
            "1  F1-Macro  0.336472\n",
            "              precision    recall  f1-score     support\n",
            "Anger          0.491736  0.743750  0.592040  160.000000\n",
            "Sadness        0.566667  0.191011  0.285714   89.000000\n",
            "Joy            0.438202  0.402062  0.419355   97.000000\n",
            "Optimism       0.076923  0.035714  0.048780   28.000000\n",
            "accuracy       0.470588  0.470588  0.470588    0.470588\n",
            "macro avg      0.393382  0.343134  0.336472  374.000000\n",
            "weighted avg   0.464627  0.470588  0.433685  374.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "torch.save(model.state_dict(), 'sentiment_cnn_model.pth')\n"
      ],
      "metadata": {
        "id": "JFlYSMRSRTkV"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load('sentiment_cnn_model.pth'))\n",
        "model.eval()\n",
        "\n",
        "# Initialize lists to store predictions and true labels\n",
        "all_predictions = []\n",
        "all_true_labels = []\n",
        "\n",
        "# Iterate through the test data\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        inputs, labels = batch\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        all_predictions.extend(predicted.tolist())\n",
        "        all_true_labels.extend(labels.tolist())\n",
        "\n",
        "# Convert the predictions and true labels to numpy arrays\n",
        "predictions = np.array(all_predictions)\n",
        "true_labels = np.array(all_true_labels)"
      ],
      "metadata": {
        "id": "UbVQ_cNaBK7p"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "\n",
        "# Calculate accuracy and F1-macro scores for test set\n",
        "accuracy = accuracy_score(true_labels, predictions)\n",
        "f1_macro = f1_score(true_labels, predictions, average='macro')\n",
        "\n",
        "# Create a classification report for detailed metrics\n",
        "report = classification_report(true_labels, predictions, target_names=['Anger', 'Sadness', 'Joy', 'Optimism'], output_dict=True)\n",
        "\n",
        "# Create a table to display the results\n",
        "results = pd.DataFrame({\n",
        "    'Metric': ['Accuracy', 'F1-Macro'],\n",
        "    'Value': [accuracy, f1_macro]\n",
        "})\n",
        "\n",
        "# Display the table\n",
        "print(results)\n",
        "\n",
        "# Display the detailed classification report\n",
        "print(pd.DataFrame(report).transpose())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WnTjem8WCBk0",
        "outputId": "f951d5f3-4407-461b-8314-d5697047e31a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Metric     Value\n",
            "0  Accuracy  0.438424\n",
            "1  F1-Macro  0.346878\n",
            "              precision    recall  f1-score      support\n",
            "Anger          0.487085  0.709677  0.577681   558.000000\n",
            "Sadness        0.479554  0.337696  0.396313   382.000000\n",
            "Joy            0.441489  0.231844  0.304029   358.000000\n",
            "Optimism       0.099338  0.121951  0.109489   123.000000\n",
            "accuracy       0.438424  0.438424  0.438424     0.438424\n",
            "macro avg      0.376866  0.350292  0.346878  1421.000000\n",
            "weighted avg   0.440010  0.438424  0.419456  1421.000000\n"
          ]
        }
      ]
    }
  ]
}